=========================================
Test Job - 1x H200 GPU
=========================================
Job started at: Fri Nov 28 04:40:11 EST 2025
Running on node: node3000
Job ID: 6729060
=========================================
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: sentencepiece in /orcd/home/002/sebasmos/.local/lib/python3.10/site-packages (0.2.1)

GPU Information:
GPU 0: NVIDIA H200 (UUID: GPU-fc904238-3259-4454-3e34-c0da0ae49b31)
GPU 1: NVIDIA H200 (UUID: GPU-5a84e17e-059a-a86d-8f5a-30d789eab9fc)

index, name, memory.total [MiB]
0, NVIDIA H200, 143771 MiB
1, NVIDIA H200, 143771 MiB
=========================================
/orcd/software/core/001/pkg/miniforge/24.3.0-0/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading model EleutherAI/gpt-neo-1.3B on cuda:0 (dtype=torch.float16) ...
Using 2 GPUs with automatic model parallelism (device_map='auto')
Multi-GPU setup with 2 GPUs:
  GPU 0: NVIDIA H200 (139.7 GB)
  GPU 1: NVIDIA H200 (139.7 GB)
Model loaded successfully.
Running with args Namespace(list_models=False, model='gpt-neo-1.3b', eval='healthbench_consensus', n_repeats=None, n_threads=1, debug=False, examples=10, quantize=None, num_gpus=2)
Loading model meta-llama/Llama-3.3-70B-Instruct on cuda:0 (dtype=torch.float16) ...
Using 2 GPUs with automatic model parallelism (device_map='auto')
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:09<04:36,  9.53s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:15<03:30,  7.51s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:23<03:30,  7.78s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:31<03:23,  7.84s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:42<03:42,  8.91s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:52<03:44,  9.34s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [01:01<03:29,  9.12s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [01:11<03:28,  9.46s/it]Loading checkpoint shards:  30%|███       | 9/30 [01:21<03:24,  9.74s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [01:31<03:14,  9.71s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [01:40<03:01,  9.55s/it]Loading checkpoint shards:  40%|████      | 12/30 [01:49<02:46,  9.24s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:55<02:24,  8.47s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [02:06<02:25,  9.08s/it]Loading checkpoint shards:  50%|█████     | 15/30 [02:15<02:17,  9.15s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [02:25<02:09,  9.26s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [02:29<01:41,  7.79s/it]Loading checkpoint shards:  60%|██████    | 18/30 [02:38<01:35,  7.98s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [02:47<01:34,  8.56s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [02:57<01:28,  8.80s/it]Loading checkpoint shards:  70%|███████   | 21/30 [03:07<01:23,  9.30s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [03:17<01:15,  9.40s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [03:27<01:07,  9.66s/it]Loading checkpoint shards:  80%|████████  | 24/30 [03:37<00:58,  9.76s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [03:47<00:48,  9.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [03:57<00:39,  9.81s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [04:05<00:27,  9.19s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [04:14<00:18,  9.37s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [04:26<00:10, 10.00s/it]Loading checkpoint shards: 100%|██████████| 30/30 [04:30<00:00,  8.23s/it]Loading checkpoint shards: 100%|██████████| 30/30 [04:30<00:00,  9.02s/it]
Multi-GPU setup with 2 GPUs:
  GPU 0: NVIDIA H200 (139.7 GB)
  GPU 1: NVIDIA H200 (139.7 GB)
Model loaded successfully.
blobfile failed (Could not find any credentials that grant access to storage account: 'openaipublic' and container: 'simple-evals'
    Access Failure: message=Could not access container, request=<Request method=GET url=https://openaipublic.blob.core.windows.net/simple-evals params={'restype': 'container', 'comp': 'list', 'maxresults': '1'}>, status=404, error=ResourceNotFound, error_description=The specified resource does not exist.
RequestId:9885e7ed-e01e-0056-0e4b-60bdc6000000
Time:2025-11-28T09:45:04.8920665Z, error_headers=Content-Length: 223, Content-Type: application/xml, Server: Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0, x-ms-request-id: 9885e7ed-e01e-0056-0e4b-60bdc6000000, x-ms-version: 2019-02-02, x-ms-error-code: ResourceNotFound, Date: Fri, 28 Nov 2025 09:45:04 GMT

No Azure credentials were found.  If the container is not marked as public, please do one of the following:

* Log in with 'az login', blobfile will use your default credentials to lookup your storage account key
* Set the environment variable 'AZURE_STORAGE_KEY' to your storage account key which you can find by following this guide: https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage
* Create an account with 'az ad sp create-for-rbac --name <name>' and set the 'AZURE_APPLICATION_CREDENTIALS' environment variable to the path of the output from that command or individually set the 'AZURE_CLIENT_ID', 'AZURE_CLIENT_SECRET', and 'AZURE_TENANT_ID' environment variables), falling back to direct HTTP download...
Downloading HealthBench data from https://openaipublic.blob.core.windows.net/simple-evals/healthbench/consensus_2025-05-09-20-00-46.jsonl...
Downloaded to /tmp/healthbench_cache/ebd5631f37bc2d0c093af1c826d05cf2db580927d3e97ea3d130c8ef30792e5d.jsonl
{'healthbench_consensus': <simple-evals.healthbench_eval.HealthBenchEval object at 0x14bb6414c820>}

Running the following evals: ['healthbench_consensus']
Running evals for the following models: ['gpt-neo-1.3b']
  0%|          | 0/10 [00:00<?, ?it/s][2025-11-28T21:28:12.002] error: *** JOB 6729060 ON node3000 CANCELLED AT 2025-11-28T21:28:12 DUE to SIGNAL Terminated ***
